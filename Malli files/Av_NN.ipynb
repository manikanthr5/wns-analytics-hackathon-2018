{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#library imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function and model imports\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mallik/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_shuffle(inp):\n",
    "    inp=shuffle(inp)\n",
    "    inp=inp.reset_index(drop=True)\n",
    "    return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54808, 14)\n",
      "(23490, 13)\n"
     ]
    }
   ],
   "source": [
    "train_data=pd.read_csv('../Data/train_LZdllcl.csv')\n",
    "test_data=pd.read_csv('../Data/test_2umaH9m.csv')\n",
    "print(train_data.shape)\n",
    "print(test_data.shape)\n",
    "#tr_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=df_shuffle(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "education               2409\n",
       "previous_year_rating    4124\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t=train_data.isnull().sum()\n",
    "t[t>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_categorical(train,test):    #use this after removing target variable\n",
    "    fulldata=pd.concat([train,test],axis=0)\n",
    "    fulldata=fulldata.fillna(-1)\n",
    "    trainend=len(train)\n",
    "    onecoded=pd.get_dummies(fulldata)\n",
    "    return (onecoded[:trainend],onecoded[trainend:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop target variable for preprocessing\n",
    "ytrainfull=train_data['is_promoted']\n",
    "X_train=train_data.drop('is_promoted',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xtrainfull,xtestproc=process_categorical(X_train,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#sample train data to test on itself\n",
    "end = 54000 \n",
    "xtrain = xtrainfull[:end]\n",
    "ytrain = ytrainfull[:end]\n",
    "\n",
    "xvalid = xtrainfull[end:54801] \n",
    "yvalid = ytrainfull[end:54801]\n",
    "xvalid=xvalid.reset_index(drop=True)\n",
    "yvalid=yvalid.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54000, 60)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X_train_std=xtrain\n",
    "X_test_std=xvalid\n",
    "X_sub_std=xtestproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_std=scale(xtrain)\n",
    "X_test_std=scale(xvalid)\n",
    "X_sub_std=scale(xtestproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_bin_train = pd.get_dummies(ytrain)\n",
    "Y_bin_test = pd.get_dummies(yvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate =0.02 #0.02\n",
    "#num_steps = 500\n",
    "batch_size = 128#128\n",
    "#display_step = 100\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 40#36 # 1st layer number of neurons\n",
    "n_hidden_2 = 30#24 # 2nd layer number of neurons\n",
    "n_hidden_3 = 15#12\n",
    "num_input = 60 # MNIST data input (img shape: 28*28)\n",
    "num_classes = 2 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "inz = tf.contrib.layers.xavier_initializer()\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([num_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'h3': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_3, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "def neural_net(x):\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    #layer_2 = tf.nn.dropout(layer_2, 0.9)\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    layer_3 = tf.add(tf.matmul(layer_2, weights['h3']), biases['b3'])\n",
    "    layer_3 = tf.nn.relu(layer_3) \n",
    "    \n",
    "    out_layer = tf.matmul(layer_3, weights['out']) + biases['out']\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct model\n",
    "logits = neural_net(X)\n",
    "\n",
    "\n",
    "class_weights = tf.constant([[1.0, 12.0]])\n",
    "# Define loss and optimizer\n",
    "#loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "#    logits=logits, labels=Y))\n",
    "loss_op = tf.reduce_mean(tf.nn.weighted_cross_entropy_with_logits(\n",
    "    logits=logits, targets=Y,pos_weight=class_weights))\n",
    "\n",
    "\n",
    "# your class weights\n",
    "#class_weights = tf.constant([[1.0, 3.0]])\n",
    "# deduce weights for batch samples based on their true label\n",
    "#weights_imb = tf.reduce_sum(class_weights * Y, axis=1)\n",
    "# compute your (unweighted) softmax cross entropy loss\n",
    "#unweighted_losses = tf.nn.softmax_cross_entropy_with_logits(logits= logits,labels=Y)\n",
    "# apply the weights, relying on broadcasting of the multiplication\n",
    "#weighted_losses = unweighted_losses * weights_imb\n",
    "# reduce the result to get your final loss\n",
    "#loss_op = tf.reduce_mean(weighted_losses)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "pred_tf=tf.argmax(logits, 1)\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "419\n",
      "Step 1, Minibatch Loss= 0.7946, Training Accuracy= 0.913\n",
      "Testing Accuracy: 0.9101124\n",
      "Step 2, Minibatch Loss= 0.7357, Training Accuracy= 0.914\n",
      "Testing Accuracy: 0.9088639\n",
      "Step 3, Minibatch Loss= 0.6725, Training Accuracy= 0.914\n",
      "Testing Accuracy: 0.9113608\n",
      "Step 4, Minibatch Loss= 0.6200, Training Accuracy= 0.914\n",
      "Testing Accuracy: 0.9101124\n",
      "Step 5, Minibatch Loss= 0.5776, Training Accuracy= 0.914\n",
      "Testing Accuracy: 0.9101124\n",
      "Step 6, Minibatch Loss= 0.5571, Training Accuracy= 0.914\n",
      "Testing Accuracy: 0.9113608\n",
      "Step 7, Minibatch Loss= 0.5413, Training Accuracy= 0.915\n",
      "Testing Accuracy: 0.9126092\n",
      "Step 8, Minibatch Loss= 0.5325, Training Accuracy= 0.911\n",
      "Testing Accuracy: 0.91635454\n",
      "Step 9, Minibatch Loss= 0.4870, Training Accuracy= 0.930\n",
      "Testing Accuracy: 0.9238452\n",
      "Step 10, Minibatch Loss= 0.4630, Training Accuracy= 0.936\n",
      "Testing Accuracy: 0.9238452\n",
      "Step 11, Minibatch Loss= 0.4499, Training Accuracy= 0.937\n",
      "Testing Accuracy: 0.92509365\n",
      "Step 12, Minibatch Loss= 0.4518, Training Accuracy= 0.938\n",
      "Testing Accuracy: 0.92883897\n",
      "Step 13, Minibatch Loss= 0.4483, Training Accuracy= 0.939\n",
      "Testing Accuracy: 0.9338327\n",
      "Step 14, Minibatch Loss= 0.4532, Training Accuracy= 0.940\n",
      "Testing Accuracy: 0.9338327\n",
      "Step 15, Minibatch Loss= 0.4684, Training Accuracy= 0.936\n",
      "Testing Accuracy: 0.9300874\n",
      "Step 16, Minibatch Loss= 0.4529, Training Accuracy= 0.929\n",
      "Testing Accuracy: 0.92883897\n",
      "Step 17, Minibatch Loss= 0.4516, Training Accuracy= 0.942\n",
      "Testing Accuracy: 0.9363296\n",
      "Step 18, Minibatch Loss= 0.4353, Training Accuracy= 0.942\n",
      "Testing Accuracy: 0.93882644\n",
      "Step 19, Minibatch Loss= 0.4317, Training Accuracy= 0.939\n",
      "Testing Accuracy: 0.9363296\n",
      "Step 20, Minibatch Loss= 0.4773, Training Accuracy= 0.938\n",
      "Testing Accuracy: 0.9338327\n",
      "Step 21, Minibatch Loss= 0.4219, Training Accuracy= 0.942\n",
      "Testing Accuracy: 0.93882644\n",
      "Step 22, Minibatch Loss= 0.4150, Training Accuracy= 0.940\n",
      "Testing Accuracy: 0.937578\n",
      "Step 23, Minibatch Loss= 0.4133, Training Accuracy= 0.943\n",
      "Testing Accuracy: 0.9400749\n",
      "Step 24, Minibatch Loss= 0.4201, Training Accuracy= 0.943\n",
      "Testing Accuracy: 0.9400749\n",
      "Step 25, Minibatch Loss= 0.4273, Training Accuracy= 0.940\n",
      "Testing Accuracy: 0.937578\n",
      "Step 26, Minibatch Loss= 0.4473, Training Accuracy= 0.942\n",
      "Testing Accuracy: 0.9400749\n",
      "Step 27, Minibatch Loss= 0.4132, Training Accuracy= 0.941\n",
      "Testing Accuracy: 0.937578\n",
      "Step 28, Minibatch Loss= 0.4194, Training Accuracy= 0.943\n",
      "Testing Accuracy: 0.9363296\n",
      "Step 29, Minibatch Loss= 0.4111, Training Accuracy= 0.943\n",
      "Testing Accuracy: 0.93882644\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.93882644\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "\n",
    "no_iterations = int(end/batch_size)-2\n",
    "print(no_iterations)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1, 30):\n",
    "        for no_batch in range(0,no_iterations):\n",
    "            \n",
    "            batch_x = X_train_std[(no_batch*batch_size) :((no_batch+1)*batch_size) ]\n",
    "            batch_y = Y_bin_train[(no_batch*batch_size) :((no_batch+1)*batch_size) ]\n",
    "            # Run optimization op (backprop)\n",
    "            sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "            #if step % display_step == 0 or step == 1:\n",
    "                # Calculate batch loss and accuracy\n",
    "        loss, acc = sess.run([loss_op, accuracy], feed_dict={X: X_train_std,Y: Y_bin_train})\n",
    "        print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                      \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                      \"{:.3f}\".format(acc))\n",
    "        print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={X: X_test_std,\n",
    "                                      Y: Y_bin_test}))\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for MNIST test images\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={X: X_test_std,\n",
    "                                      Y: Y_bin_test}))\n",
    "    test_pred= sess.run(pred_tf, feed_dict={X: X_test_std,Y: Y_bin_test})\n",
    "    tr_pred= sess.run(pred_tf, feed_dict={X: X_train_std,Y: Y_bin_test})\n",
    "    sub_pred= sess.run(pred_tf, feed_dict={X: X_sub_std})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9430740740740741\n",
      "precision: [0.94356893 0.92853123]\n",
      "recall: [0.99742931 0.35892974]\n",
      "fscore: [0.96975184 0.51772827]\n"
     ]
    }
   ],
   "source": [
    "precision, recall, fscore, support = score(ytrain, tr_pred)\n",
    "accuracy = accuracy_score(ytrain,tr_pred)\n",
    "print('Accuracy: {}'.format(accuracy))\n",
    "\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "#print('support: {}'.format(support))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9388264669163545\n",
      "precision: [0.94056848 0.88888889]\n",
      "recall: [0.99589603 0.34285714]\n",
      "fscore: [0.96744186 0.49484536]\n"
     ]
    }
   ],
   "source": [
    "precision, recall, fscore, support = score(yvalid, test_pred)\n",
    "accuracy = accuracy_score(yvalid,test_pred)\n",
    "print('Accuracy: {}'.format(accuracy))\n",
    "\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "#print('support: {}'.format(support))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_data= {'employee_id':xtestproc['employee_id'],'is_promoted':sub_pred}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(sub_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.to_csv(\"../results/pred3normal_nn.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'stop'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-44b759c40b9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'stop'"
     ]
    }
   ],
   "source": [
    "import stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xtrain.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rf = RandomForestClassifier(n_estimators = 20, random_state = 0,max_depth=None,min_samples_split=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rf = DecisionTreeClassifier(criterion = \"gini\", max_depth=None,min_samples_split=4,random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = svm.SVC(kernel='linear', C=500, gamma=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.fit(xtrain,ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_pred = rf.predict(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_pred = rf.predict(xvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, fscore, support = score(ytrain, tr_pred)\n",
    "accuracy = accuracy_score(ytrain,tr_pred)\n",
    "print('Accuracy: {}'.format(accuracy))\n",
    "\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "#print('support: {}'.format(support))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, fscore, support = score(yvalid, valid_pred)\n",
    "accuracy = accuracy_score(yvalid,valid_pred)\n",
    "print('Accuracy: {}'.format(accuracy))\n",
    "\n",
    "print('precision: {}'.format(precision))\n",
    "print('recall: {}'.format(recall))\n",
    "print('fscore: {}'.format(fscore))\n",
    "#print('support: {}'.format(support))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data['is_promoted'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_data['department'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "l1=ts_data['employee_id'].values\n",
    "l2=tr_data['employee_id'].values\n",
    "common=np.intersect1d(l1,l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(l1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(l2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
